{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: SQLite3 version 3.40.0 and 3.41.2 have huge performance regressions; please install version 3.41.1 or 3.42!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.taxo_utils import taxonomy\n",
    "np.random.seed(114514)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './../data/raw/google.json'\n",
    "data = taxonomy.from_json(path)\n",
    "dataset_name = re.findall(r'/(\\w+).json$',path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following methods are used to build the seq2seq data from taxonomy.\n",
    "# Obtain clusters from an taxonomy. Each cluster is a list of classes sharing the same common parent.\n",
    "# For the current dataset, each cluster is the set of direct children and grandchildren of a certain class.\n",
    "def get_clusters_from_taxonomy(taxo:taxonomy,shuffle=True):\n",
    "    clusters = []\n",
    "    for cat in taxo.nodes():\n",
    "        if cat == 0:\n",
    "            continue\n",
    "        family = taxo.get_descendants_by_depth(cat,max_depth=2)\n",
    "        if len(family) <= 1:\n",
    "            continue\n",
    "        if shuffle:\n",
    "            np.random.shuffle(family)\n",
    "        clusters.append(family)            \n",
    "    return clusters\n",
    "\n",
    "# Create \"negative\" samples by inserting random classes into clusters. A pattern is a tuple (npos,nneg) where npos classes are taken from a proper cluster, while nneg classes are randomly inserted.\n",
    "def fill_cluster_with_neg(taxo,clusters,pattern,n):\n",
    "    npos,nneg = pattern\n",
    "    total = npos + nneg\n",
    "    filled_clusters = []\n",
    "    useable_clusters = [c for c in clusters if len(c) >= npos] if npos > 0 else clusters\n",
    "    N = len(useable_clusters)\n",
    "    classlist = list(taxo.nodes())\n",
    "    taxo_size = taxo.number_of_nodes()\n",
    "    with tqdm(total = n, desc=f'Negative pattern ({npos},{nneg})') as pbar:\n",
    "        while len(filled_clusters) < n:\n",
    "            randcluster = useable_clusters[int(np.random.choice(N))]\n",
    "            size = len(randcluster)\n",
    "            indices = np.random.choice(size,npos,replace=False) if npos > 0 else []\n",
    "            randcluster = [randcluster[i] for i in indices]\n",
    "            need_check = 1\n",
    "            while len(randcluster) < total:\n",
    "                randclass = classlist[int(np.random.choice(taxo_size))]\n",
    "                if randclass in randcluster:\n",
    "                    continue\n",
    "                elif len(randcluster) == 0 or not need_check:\n",
    "                        randcluster.append(randclass)\n",
    "                elif taxo.get_LCA(randcluster + [randclass]) == [0]:\n",
    "                    need_check = 0\n",
    "                    randcluster.append(randclass)\n",
    "                else:\n",
    "                    continue\n",
    "            if randcluster not in filled_clusters:\n",
    "                filled_clusters.append(randcluster)\n",
    "                pbar.update(1)\n",
    "    return list(filled_clusters)\n",
    "\n",
    "# Split the cluster into smaller sizes in order to keep the model input at reasonable length.\n",
    "def split_cluster(cluster,max_chunk_size=3):\n",
    "    total = len(cluster)\n",
    "    progress = 0\n",
    "    miniclusters = []\n",
    "    while progress < total:\n",
    "        if (total - progress) <= max_chunk_size:\n",
    "            next_size = total - progress\n",
    "        else:\n",
    "            next_size = np.random.randint(2,max_chunk_size+1)\n",
    "        miniclusters.append(cluster[progress:(progress+next_size)])\n",
    "        progress += next_size  \n",
    "    return miniclusters\n",
    "    \n",
    "# Formulate the sentences that we feed into the seq2seq model.\n",
    "def build_prompt(taxo, classes, shuffle=False,prefix='summarize: '):\n",
    "    if shuffle:\n",
    "        np.random.shuffle(classes)\n",
    "    prompt = prefix\n",
    "    for c in classes:\n",
    "        prompt = prompt + taxo.get_label(c) + '; '\n",
    "    return prompt[:-2]\n",
    "\n",
    "# Create a DataFrame for the raw data.\n",
    "def write_to_pandas(taxo,clusters):\n",
    "    data = {'text':[],'summary':[]}\n",
    "    for cluster in clusters:\n",
    "        data['summary'].append(taxo.nodes[taxo.get_LCA(cluster)[0]]['label'])\n",
    "        data['text'].append(build_prompt(taxo,cluster))\n",
    "    return pd.DataFrame(data)\n",
    "            \n",
    "# Seq2seq data for the training of text generation model.\n",
    "def build_seq2seq_data(taxo,chunk_size=3,neg2pos_ratio=0.4,neg_patterns=[(0,2),(0,3),(2,1)],pattern_weight=[1,1,3],test_size=0.1):\n",
    "    clusters = get_clusters_from_taxonomy(taxo)\n",
    "    miniclusters = []\n",
    "    dflist = []\n",
    "    for cluster in clusters:\n",
    "        miniclusters += split_cluster(cluster,chunk_size)\n",
    "    dflist.append(write_to_pandas(taxo,miniclusters))\n",
    "    N = len(miniclusters)\n",
    "    if neg2pos_ratio > 0:\n",
    "        pattern_weight = np.array(pattern_weight)\n",
    "        pattern_weight = pattern_weight / np.sum(pattern_weight)\n",
    "        for i, pattern in enumerate(neg_patterns):\n",
    "            dflist.append(write_to_pandas(taxo,fill_cluster_with_neg(taxo,clusters,pattern,int(np.ceil(N * neg2pos_ratio * pattern_weight[i])))))\n",
    "    test_data = [df.sample(frac=test_size) for df in dflist]\n",
    "    train_data = [dflist[i].drop(testdf.index) for i,testdf in enumerate(test_data)]\n",
    "    return pd.concat(train_data), pd.concat(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a600fe29219467bbfc24134f6d5ae76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Negative pattern (0,2):   0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57da227197734045907d74b0742dff7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Negative pattern (0,3):   0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ab060413a247afbdd7516a32e4340b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Negative pattern (2,1):   0%|          | 0/1068 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data,eval_data = build_seq2seq_data(data)\n",
    "train_data.to_csv(f'./../data/gen/{dataset_name}-train.csv',index=False)\n",
    "eval_data.to_csv(f'./../data/gen/{dataset_name}-test.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
