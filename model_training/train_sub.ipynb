{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "371e4970-b607-49a2-af82-ccaa7a2fd9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa7b7ce6-d23a-445e-9f01-a57c68f7b7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6c4dec7990>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = './../../data/sub/train.csv'\n",
    "test_data_path = './../../data/sub/test.csv'\n",
    "model_checkpoint = \"/data2T/jingchuan/untuned/ebert/base/\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_batch_size = 64\n",
    "eval_batch_size = 64\n",
    "num_train_epochs = 3\n",
    "lr = 1.5e-5\n",
    "lr_schedule='linear'\n",
    "np.random.seed(114514)\n",
    "torch.manual_seed(114514)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acd9d6b5-3fd1-4a36-949e-7ba434e88792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /data2T/jingchuan/untuned/ebert/base/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'EBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_checkpoint,model_max_length=72)\n",
    "data_collator = DataCollatorWithPadding(tokenizer,padding=True)\n",
    "precision_score = evaluate.load('precision')\n",
    "recall_score = evaluate.load('recall')\n",
    "f1_score = evaluate.load(\"f1\")\n",
    "acc_score = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca8f8726-5c82-412a-9e12-e5d612ee5030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples,return_tensors=None):\n",
    "    return tokenizer(examples[\"Subclass\"],examples[\"Superclass\"],return_tensors=return_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0460d703-a242-4e6b-8d4a-12dc8c518275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = acc_score.compute(predictions=predictions, references=labels)\n",
    "    p = precision_score.compute(predictions=predictions, references=labels)\n",
    "    r = recall_score.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_score.compute(predictions=predictions, references=labels)\n",
    "    result = {**acc, **p, **r, **f1}\n",
    "    return {k: round(v, 6) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87c6ad37-c4cb-446f-8e50-8aaa6fcebdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd62bc586a8f4f85acab33c74ed7b73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/242229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f464d7965840ceaef4342e09d93a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_data_path)\n",
    "eval_data = pd.read_csv(test_data_path)\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "eval_dataset = Dataset.from_pandas(eval_data)\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\",\"token_type_ids\",\"labels\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\",\"token_type_ids\",\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32ceabf9-c6eb-4ba4-8482-43b507533c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "timestr = now.strftime('%Y%m%d-%H%M')\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"/data2T/jingchuan/tuned/sub/{timestr}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type=lr_schedule,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1c05dc2-d61f-4a2f-9a2e-e66dc6150eef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jingchuan/anaconda3/envs/ICON/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5679' max='5679' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5679/5679 2:08:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.047041</td>\n",
       "      <td>0.983216</td>\n",
       "      <td>0.973265</td>\n",
       "      <td>0.976471</td>\n",
       "      <td>0.974865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.049538</td>\n",
       "      <td>0.986039</td>\n",
       "      <td>0.967616</td>\n",
       "      <td>0.991294</td>\n",
       "      <td>0.979312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.042683</td>\n",
       "      <td>0.987922</td>\n",
       "      <td>0.973198</td>\n",
       "      <td>0.991059</td>\n",
       "      <td>0.982047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jingchuan/anaconda3/envs/ICON/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jingchuan/anaconda3/envs/ICON/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5679, training_loss=0.05752038527467045, metrics={'train_runtime': 7737.9876, 'train_samples_per_second': 93.912, 'train_steps_per_second': 0.734, 'total_flos': 1.042452990223728e+16, 'train_loss': 0.05752038527467045, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_outputs = trainer.train()\n",
    "now = datetime.now()\n",
    "timestr = now.strftime('%Y%m%d-%H%M')\n",
    "training_outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ba59333-9c39-4a5b-8a22-748ddf138b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.7969e-04, 9.9952e-01])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = {'Subclass':'Vintage Clothing', 'Superclass':'Women\\'s Clothing, Shoes & Accessories'}\n",
    "inputs = tokenize(example,return_tensors='pt').to(device)\n",
    "predictions = torch.softmax(model(**inputs).logits.detach().cpu().squeeze(),0)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04b50321-453f-43bf-9c78-ed5b123f94d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data2T/jingchuan/tuned/sub/bertsubs-sota/tokenizer_config.json',\n",
       " '/data2T/jingchuan/tuned/sub/bertsubs-sota/special_tokens_map.json',\n",
       " '/data2T/jingchuan/tuned/sub/bertsubs-sota/vocab.txt',\n",
       " '/data2T/jingchuan/tuned/sub/bertsubs-sota/added_tokens.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('/data2T/jingchuan/tuned/sub/bertsubs-sota')\n",
    "tokenizer.save_pretrained('/data2T/jingchuan/tuned/sub/bertsubs-sota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7834d9c1-4b7f-4b95-a22b-6cd4d8f6886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jingchuan/Taxonomy_Completion-main/experiments/training\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
