{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e4970-b607-49a2-af82-ccaa7a2fd9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7b7ce6-d23a-445e-9f01-a57c68f7b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './../../data/gen/google-train.csv'\n",
    "test_data_path = './../../data/gen/google-test.csv'\n",
    "model_checkpoint = 'YOUR_MODEL_CHECKPOINT'\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "num_train_epochs = 10\n",
    "lr = 2e-5\n",
    "lr_schedule='linear'\n",
    "max_gen_length = 64\n",
    "np.random.seed(114514)\n",
    "torch.manual_seed(114514)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9d6b5-3fd1-4a36-949e-7ba434e88792",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,model_max_length=128)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "bertscore = evaluate.load('bertscore')\n",
    "gleu_score = evaluate.load(\"google_bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    model_inputs = tokenizer(examples[\"text\"])\n",
    "    labels = tokenizer(examples[\"summary\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460d703-a242-4e6b-8d4a-12dc8c518275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    gleu = gleu_score.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "    bscore = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang='en')\n",
    "    bscore['Bs-P'] = np.mean(np.array(bscore.pop('precision'))).round(6)\n",
    "    bscore['Bs-R'] = np.mean(np.array(bscore.pop('recall'))).round(6)\n",
    "    bscore['Bs-F1'] = np.mean(np.array(bscore.pop('f1'))).round(6)\n",
    "    bscore.pop('hashcode')\n",
    "    result = {**bscore, **gleu}\n",
    "    return {k: round(v, 6) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6ad37-c4cb-446f-8e50-8aaa6fcebdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path)\n",
    "eval_data = pd.read_csv(test_data_path)\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "eval_dataset = Dataset.from_pandas(eval_data)\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\",\"labels\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\",\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ceabf9-c6eb-4ba4-8482-43b507533c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "timestr = now.strftime('%Y%m%d-%H%M')\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f'YOUR_PATH/{model_name}-{timestr}',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type=lr_schedule,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=8,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    generation_max_length=max_gen_length,\n",
    "    predict_with_generate=True,\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c05dc2-d61f-4a2f-9a2e-e66dc6150eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_outputs = trainer.train()\n",
    "now = datetime.now()\n",
    "timestr = now.strftime('%Y%m%d-%H%M')\n",
    "training_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98512365",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.evaluation_loop(\n",
    "            trainer.get_eval_dataloader(eval_dataset),\n",
    "            description=\"Evaluation\"\n",
    "        )\n",
    "preds = np.where(output.predictions != -100, output.predictions, tokenizer.pad_token_id)\n",
    "predictions = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "eval_result_table = pd.DataFrame({'Input':list(map(lambda string: string[11:], eval_dataset['text'])), # Strip the \"summarize: \" prefix\n",
    "                                'Prediction': predictions, \n",
    "                                'Reference': eval_dataset['summary']})\n",
    "print(output.metrics)\n",
    "eval_result_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
