{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a27217",
   "metadata": {},
   "source": [
    "# ICON demonstration\n",
    "\n",
    "This notebook is a guided example of using ICON to enrich the Google Product Type Taxonomy.\n",
    "Before running this notebook, make sure that you have read README.md of the ICON repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441463b",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "**Replace SimCSE script**: For the purpose of this demonstration, please temporarily replace the `tool.py` in your SimCSE directory with `/utils/replace_simcse/tool.py`. The reasons are explained [here](/README.md#replace-simcse-script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d25888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip show simcse | grep -P \"Location: .*$\" # Locate your SimCSE package. \n",
    "# Copy the directory given by the above command's outputs, which will look like:\n",
    "    # Location: SIMCSE_DIR\n",
    "# Now uncomment the following line and replace SIMCSE_DIR with what you have copied\n",
    "# ! cp utils/replace_simcse/tool.py /home/jingcshi/.conda/envs/icon/lib/python3.8/site-packages/simcse/tool.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8512b",
   "metadata": {},
   "source": [
    "## Importing relevant packages\n",
    "\n",
    "A complete list of dependencies is available in the [README](/README.md#dependencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10dff60-49d0-47a6-828c-937eec5a79b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union, Hashable\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from simcse import SimCSE\n",
    "from ellement.transformers import AutoModel, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from utils import taxo_utils\n",
    "from utils.taxo_utils import Taxonomy\n",
    "from main.icon import ICON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e9d0c",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "The taxonomy dataset will be loaded as a `utils.taxo_utils.Taxonomy` object. For I/O format details, please refer to the corresponding section in [README](README.md#file-io-format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbdea1-25a3-4000-b687-c91029617976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "taxo = taxo_utils.from_json('./data/raw/google.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ce650",
   "metadata": {},
   "source": [
    "## Loading the models\n",
    "\n",
    "ICON requires three sub-models: `ret_model`, `gen_model` and `sub_model`.\n",
    "\n",
    "**If you don't have these models**: The scripts in `/experiments/data_wrangling/` and notebooks in `/experiments/model_training/` will offer a pipeline for preparing the training data and fine-tuning pre-trained language models.\n",
    "\n",
    "**Models for eBay**: Models fine-tuned on eBay data with the pipeline described below are available at RNO HDFS: `/user/jingcshi/ICON_models/`.\n",
    "\n",
    "Our choices of ret_model, gen_model and sub_model each requires a tokenizer. The tokenizer for ret_model is automatically loaded during the SimCSE init command.\n",
    "\n",
    "Notice that ICON uses its sub-models as callable functions and doesn't care how the models themselves are implemented. Therefore, we need to wrap these models in callable interfaces. This will be demonstrated in a [cell below](#wrapping-the-models-as-callables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495a24e-6126-4bb4-907b-aefbdce9c0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ret_model_path = 'YOUR_MODEL_PATH'\n",
    "gen_model_path = 'YOUR_MODEL_PATH'\n",
    "sub_model_path = 'YOUR_MODEL_PATH'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa9286",
   "metadata": {},
   "source": [
    "## Wrapping the models as callables classes\n",
    "\n",
    "Here we create a class for each sub-model with a `__call__` method so that ICON can directly call them.\n",
    "\n",
    "Each model has its expected inputs and outputs:\n",
    "\n",
    "- `RET_model`: Takes in a list of concepts, a query string (the concepts most similar to which we would like to find out), and an integer `k`, the amount of concepts to be retrieved. Returns a list of concept IDs in the taxonomy.\n",
    "\n",
    "- `GEN_model`: Takes in a list of strings (concept labels which the model should summarise). Returns a single string (label for the union concept).\n",
    "\n",
    "- `SUB_model`: Takes in two lists of strings (the labels for `sub` and `sup` respectively). Returns an 1D array of prediction scores of how likely each concept in `sup` subsumes the corresponding concept in `sub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060cdda-38b8-4abb-bb11-b8d9c29269e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RET_model:\n",
    "\n",
    "    def __init__(self, model_path, **kwargs) -> None:\n",
    "        self.model = SimCSE(model_path, **kwargs)\n",
    "        self.idx_dict = {}\n",
    "    \n",
    "    def build_index(self, concepts: List[Hashable], **kwargs) -> None:\n",
    "        self.model.build_index(taxo.get_label(concepts), **kwargs)\n",
    "        self.idx_dict = {i: c for i, c in enumerate(concepts)}\n",
    "\n",
    "    def __call__(self, concepts: List[Hashable], query: str, k: int=10) -> List[Hashable]:\n",
    "        if self.model.index is None:\n",
    "            self.build_index(concepts)\n",
    "        if set(concepts) != set(self.model.index['sentences']):\n",
    "            self.build_index(concepts)\n",
    "        ans = self.model.search(query, top_k=k)\n",
    "        return [self.idx_dict[i] for i,_,_ in ans]\n",
    "    \n",
    "    def similarity(self, query, keys) -> np.ndarray:\n",
    "        if self.model.index is None:\n",
    "            key_embeds = self.model.encode(keys, return_numpy=True)\n",
    "        else:\n",
    "            key_embeds = []\n",
    "            n = self.model.index['index'].ntotal\n",
    "            d = self.model.index['index'].d\n",
    "            for k in keys:\n",
    "                try:\n",
    "                    key_embeds.append(faiss.rev_swig_ptr(self.model.index['index'].get_xb(), n*d).reshape(n, d)[self.model.index['sentences'].index(k)])\n",
    "                except ValueError:\n",
    "                    key_embeds.append(self.model.encode(k, return_numpy=True))\n",
    "            key_embeds = np.stack(key_embeds)\n",
    "        return self.model.similarity(query, key_embeds)\n",
    "\n",
    "class GEN_model:\n",
    "\n",
    "    def __init__(self, model_path, **kwargs) -> None:\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path, **kwargs).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.max_length = self.model.config.max_length\n",
    "\n",
    "    def __call__(self, labels: List[str], prefix='summarize: ') -> str:\n",
    "        corpus = prefix\n",
    "        for l in labels:\n",
    "            corpus += l + '[SEP]'\n",
    "        corpus = corpus[:-5]\n",
    "        inputs = self.tokenizer(corpus,return_tensors='pt').to(device)['input_ids']\n",
    "        outputs = self.model.generate(inputs,max_length=self.max_length)[0]\n",
    "        decoded = self.tokenizer.decode(outputs.cpu().numpy(),skip_special_tokens=True)\n",
    "        return decoded\n",
    "\n",
    "class SUB_model:\n",
    "\n",
    "    def __init__(self, model_path, **kwargs) -> None:\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path, **kwargs).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path,model_max_length=128)\n",
    "\n",
    "    def __call__(self, sub: Union[str, List[str]], sup: Union[str, List[str]], batch_size :int=256) -> np.ndarray:\n",
    "        if isinstance(sub, str):\n",
    "            sub, sup = [sub], [sup]\n",
    "        if len(sub) <= batch_size:\n",
    "            inputs = self.tokenizer(sub,sup,padding=True,return_tensors='pt').to(device)\n",
    "            predictions = torch.softmax(self.model(**inputs).logits.detach().cpu(),1)[:,1].numpy()\n",
    "        else:\n",
    "            head = (sub[:batch_size], sup[:batch_size])\n",
    "            tail = (sub[batch_size:],sup[batch_size:])\n",
    "            predictions = np.concatenate((SUB_model(head[0], head[1], batch_size=batch_size), SUB_model(tail[0], tail[1], batch_size=batch_size)))\n",
    "        return predictions\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ret_model = RET_model(ret_model_path, device=device, pooler=\"cls_before_pooler\")\n",
    "gen_model = GEN_model(gen_model_path, max_length=64)\n",
    "sub_model = SUB_model(sub_model_path)\n",
    "ret_model.build_index(list(taxo.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682f1b5",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Almost there! Configure your run by specifying the data, models and settings. Check [here](/README.md#configurations) to see how to choose the right settings for your purpose. \n",
    "\n",
    "In the following example, we will run auto mode with 10 outer loops. We will also set `logging` to `True` to see a detailed logging of ICON's actions and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fac59b-a9f5-4e24-a463-160967c6401b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kwargs = {'data': taxo,\n",
    "        'ret_model': RET_model,\n",
    "        'gen_model': GEN_model,\n",
    "        'sub_model': SUB_model,\n",
    "        'restrict_combinations': False,\n",
    "        'retrieve_size': 5,\n",
    "        'logging': 1}\n",
    "\n",
    "iconobj = ICON(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e371c9f",
   "metadata": {},
   "source": [
    "## Running\n",
    "\n",
    "We have prepared everything to run ICON. Simply initialise an ICON object with our configuration and call `run()`. \n",
    "\n",
    "If you change your mind on the settings before running, you don't have to initialise again: calling `update_config` would suffice.\n",
    "\n",
    "The output of a run will be either a new taxonomy (as is the case here) or a list of ICON predictions. To save a taxonomy to a file, use the `to_json` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8edbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "iconobj.update_config(threshold=0.8, logging=True, subgraph_strict=False) # Example of updating configurations\n",
    "outputs = iconobj.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
