{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a27217",
   "metadata": {},
   "source": [
    "# ICON demonstration\n",
    "\n",
    "This notebook is a guided example of using ICON to enrich the Google Product Type Taxonomy.\n",
    "Before running this notebook, make sure that you have read README.md of the ICON repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8512b",
   "metadata": {},
   "source": [
    "## Importing relevant packages\n",
    "\n",
    "A complete list of dependencies is available in the [README](/README.md#dependencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10dff60-49d0-47a6-828c-937eec5a79b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union, Hashable\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ellement.transformers import AutoModel, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from utils import taxo_utils\n",
    "from utils.taxo_utils import Taxonomy\n",
    "from main.icon import ICON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e9d0c",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "The taxonomy dataset will be loaded as a `utils.taxo_utils.Taxonomy` object. For I/O format details, please refer to the corresponding section in [README](README.md#file-io-format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbdea1-25a3-4000-b687-c91029617976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "taxo = taxo_utils.from_json('./data/raw/google.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ce650",
   "metadata": {},
   "source": [
    "## Loading the models\n",
    "\n",
    "ICON requires three sub-models: `emb_model`, `gen_model` and `sub_model`.\n",
    "\n",
    "**If you don't have these models**: The scripts in `/experiments/data_wrangling/` and notebooks in `/experiments/model_training/` will offer a pipeline for preparing the training data and fine-tuning pre-trained language models.\n",
    "\n",
    "**Models for eBay**: Models fine-tuned on eBay data with the pipeline described below are available at RNO HDFS: `/user/jingcshi/ICON_models/`.\n",
    "\n",
    "Our choices of emb_model, gen_model and sub_model each requires a tokenizer. The tokenizer for ret_model is automatically loaded during the SimCSE init command.\n",
    "\n",
    "Notice that ICON uses its sub-models as callable functions and doesn't care how the models themselves are implemented. Therefore, we need to wrap these models in callable interfaces. This will be demonstrated in a [cell below](#wrapping-the-models-as-callables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495a24e-6126-4bb4-907b-aefbdce9c0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ret_model_path = 'YOUR_MODEL_PATH'\n",
    "gen_model_path = 'YOUR_MODEL_PATH'\n",
    "sub_model_path = 'YOUR_MODEL_PATH'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa9286",
   "metadata": {},
   "source": [
    "## Wrapping the models as callables classes\n",
    "\n",
    "Here we create a class for each sub-model with a `__call__` method so that ICON can directly call them.\n",
    "\n",
    "Each model has its expected inputs and outputs:\n",
    "\n",
    "- `EMB_model`: Takes in one or a list of sentences (strings). Returns a numpy array representing the embeddings of each sentence. \n",
    "\n",
    "- `GEN_model`: Takes in a list of strings (concept labels which the model should summarise). Returns a single string (label for the union concept).\n",
    "\n",
    "- `SUB_model`: Takes in two lists of strings (the labels for `sub` and `sup` respectively). Returns an 1D array of prediction scores of how likely each concept in `sup` subsumes the corresponding concept in `sub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060cdda-38b8-4abb-bb11-b8d9c29269e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMB_model:\n",
    "\n",
    "    def __init__(self, model_path, **kwargs) -> None:\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = AutoModel.from_pretrained(model_path, **kwargs).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    def __call__(self, sentence: Union[str, List[str]], batch_size: int=64, max_length: int=64, normalize: bool = True) -> np.ndarray:\n",
    "\n",
    "        single_sentence = False\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = [sentence]\n",
    "            single_sentence = True\n",
    "        \n",
    "        embedding_list = []\n",
    "        with torch.no_grad():\n",
    "            total_batch = len(sentence) // batch_size + (1 if len(sentence) % batch_size > 0 else 0)\n",
    "            for batch_id in range(total_batch):\n",
    "                inputs = self.tokenizer(\n",
    "                    sentence[batch_id*batch_size:(batch_id+1)*batch_size], \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=max_length, \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                outputs = self.model(**inputs, return_dict=True).last_hidden_state[:, -1]\n",
    "                embedding_list.append(outputs.cpu())\n",
    "        embeddings = torch.cat(embedding_list, 0)\n",
    "        if normalize:\n",
    "            embeddings = embeddings / torch.norm(embeddings, p=2, dim=1, keepdim=True)\n",
    "        if single_sentence:\n",
    "            embeddings = embeddings[0]\n",
    "        return embeddings.numpy()\n",
    "\n",
    "class GEN_model:\n",
    "\n",
    "    def __init__(self, model_path, **kwargs) -> None:\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path, **kwargs).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.max_length = self.model.config.max_length\n",
    "\n",
    "    def __call__(self, labels: List[str], prefix='summarize: ') -> str:\n",
    "        corpus = prefix\n",
    "        for l in labels:\n",
    "            corpus += l + '[SEP]'\n",
    "        corpus = corpus[:-5]\n",
    "        inputs = self.tokenizer(corpus,return_tensors='pt').to(device)['input_ids']\n",
    "        outputs = self.model.generate(inputs,max_length=self.max_length)[0]\n",
    "        decoded = self.tokenizer.decode(outputs.cpu().numpy(),skip_special_tokens=True)\n",
    "        return decoded\n",
    "\n",
    "class SUB_model:\n",
    "\n",
    "    def __init__(self, model_path, **kwargs) -> None:\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path, **kwargs).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path,model_max_length=128)\n",
    "\n",
    "    def __call__(self, sub: Union[str, List[str]], sup: Union[str, List[str]], batch_size :int=256) -> np.ndarray:\n",
    "        if isinstance(sub, str):\n",
    "            sub, sup = [sub], [sup]\n",
    "        if len(sub) <= batch_size:\n",
    "            inputs = self.tokenizer(sub,sup,padding=True,return_tensors='pt').to(device)\n",
    "            predictions = torch.softmax(self.model(**inputs).logits.detach().cpu(),1)[:,1].numpy()\n",
    "        else:\n",
    "            head = (sub[:batch_size], sup[:batch_size])\n",
    "            tail = (sub[batch_size:],sup[batch_size:])\n",
    "            predictions = np.concatenate((SUB_model(head[0], head[1], batch_size=batch_size), SUB_model(tail[0], tail[1], batch_size=batch_size)))\n",
    "        return predictions\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ret_model = EMB_model(ret_model_path)\n",
    "gen_model = GEN_model(gen_model_path, max_length=64)\n",
    "sub_model = SUB_model(sub_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682f1b5",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Almost there! Configure your run by specifying the data, models and settings. Check [here](/README.md#configurations) to see how to choose the right settings for your purpose. \n",
    "\n",
    "In the following example, we will run auto mode with 10 outer loops. We will also set `logging` to `True` to see a detailed logging of ICON's actions and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fac59b-a9f5-4e24-a463-160967c6401b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kwargs = {'data': taxo,\n",
    "        'emb_model': EMB_model,\n",
    "        'gen_model': GEN_model,\n",
    "        'sub_model': SUB_model,\n",
    "        'restrict_combinations': False,\n",
    "        'retrieve_size': 5,\n",
    "        'logging': 1}\n",
    "\n",
    "iconobj = ICON(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e371c9f",
   "metadata": {},
   "source": [
    "## Running\n",
    "\n",
    "We have prepared everything to run ICON. Simply initialise an ICON object with our configuration and call `run()`. \n",
    "\n",
    "If you change your mind on the settings before running, you don't have to initialise again: calling `update_config` would suffice.\n",
    "\n",
    "The output of a run will be either a new taxonomy (as is the case here) or a list of ICON predictions. To save a taxonomy to a file, use the `to_json` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8edbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "iconobj.update_config(threshold=0.8, logging=True, subgraph_strict=False) # Example of updating configurations\n",
    "outputs = iconobj.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
