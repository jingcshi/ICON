{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a27217",
   "metadata": {},
   "source": [
    "# ICON demonstration\n",
    "\n",
    "This notebook is a guided example of using ICON to enrich the Google Product Type Taxonomy.\n",
    "Before running this notebook, make sure that you have read README.md of the ICON repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441463b",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "**Replace SimCSE script**: For the purpose of this demonstration, please temporarily replace the `tool.py` in your SimCSE directory with `/utils/replace_simcse/tool.py`. The reasons are explained [here](/README.md#replace-simcse-script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d25888",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip show simcse | grep -P \"Location: .*$\" # Locate your SimCSE package. \n",
    "# Copy the directory given by the above command's outputs, which will look like:\n",
    "    # Location: SIMCSE_DIR\n",
    "# Now uncomment the following line and replace SIMCSE_DIR with what you have copied\n",
    "# ! cp utils/replace_simcse/tool.py SIMCSE_DIR/simcse/tool.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8512b",
   "metadata": {},
   "source": [
    "## Importing relevant packages\n",
    "\n",
    "A complete list of dependencies is available in the [README](/README.md#dependencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10dff60-49d0-47a6-828c-937eec5a79b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Hashable\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from simcse import SimCSE\n",
    "from transformers import BertForSequenceClassification, AutoModelForSeq2SeqLM, BertTokenizer, AutoTokenizer\n",
    "from utils import taxo_utils\n",
    "from utils.taxo_utils import Taxonomy\n",
    "from main.icon import ICON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ce650",
   "metadata": {},
   "source": [
    "## Loading the models\n",
    "\n",
    "ICON requires three sub-models: `ret_model`, `gen_model` and `sub_model`.\n",
    "\n",
    "**If you don't have these models**: The notebooks in `/data_wrangling/` and `/model_training/` will offer a pipeline for preparing the training data and fine-tuning pre-trained language models.\n",
    "\n",
    "**Models for eBay**: Models fine-tuned on eBay data with the pipeline described below are available at RNO: `/user/jingcshi/ICON_models/`.\n",
    "\n",
    "Our choices of ret_model, gen_model and sub_model each requires a tokenizer. The tokenizer for ret_model is automatically loaded during the SimCSE() command.\n",
    "\n",
    "Notice that ICON uses its sub-models as callable functions and doesn't care how the models themselves are implemented. Therefore, we need to wrap these models in callable interfaces. This will be demonstrated in a [cell below](#wrapping-the-models-as-callables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495a24e-6126-4bb4-907b-aefbdce9c0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ret_model = SimCSE('/your/path/to/ret_model',device=device)\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained('/your/path/to/gen_model').to(device)\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained('/your/path/to/gen_model')\n",
    "sub_model = BertForSequenceClassification.from_pretrained('/your/path/to/sub_model').to(device)\n",
    "sub_tokenizer = BertTokenizer.from_pretrained('/your/path/to/sub_model',model_max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e9d0c",
   "metadata": {},
   "source": [
    "## Reading and preprocessing data\n",
    "\n",
    "The taxonomy dataset will be loaded as a `utils.taxo_utils.Taxonomy` object.\n",
    "\n",
    "We can get a tabular view of the dataset by converting its concepts into a pandas DataFrame. This DataFrame will also be used to track the index of each concept when the concepts are converted to a flat list (which will happen when SimCSE returns its results later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbdea1-25a3-4000-b687-c91029617976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "taxo = taxo_utils.from_json('./data/raw/google.json')\n",
    "df = pd.DataFrame(taxo.nodes(data='label'),columns=['ID','Label']).drop(0).reset_index(drop=True)\n",
    "idx_dict = {}\n",
    "for i,row in df.iterrows():\n",
    "    idx_dict[i] = row['ID'] # Convert a concept's index in the flat list to its ID in the taxonomy.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa9286",
   "metadata": {},
   "source": [
    "## Wrapping the models as callables\n",
    "\n",
    "Here we create a function for each sub-model so that ICON can directly call them.\n",
    "\n",
    "Each function has its expected inputs and outputs:\n",
    "\n",
    "- `RET_model`: Takes in a taxonomy, a query string (the concepts most similar to which we would like to find out), and an integer `k`, the amount of concepts to be retrieved. Returns a list of concept IDs in the taxonomy.\n",
    "\n",
    "- `GEN_model`: Takes in a list of strings (concept labels which the model should summarise). Returns a single string (label for the union concept).\n",
    "\n",
    "- `SUB_model`: Takes in two lists of strings (the labels for `sub` and `sup` respectively). Returns an 1D array of prediction scores of how likely each concept in `sup` subsumes the corresponding concept in `sub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060cdda-38b8-4abb-bb11-b8d9c29269e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_model.build_index(list(df['Label']))\n",
    "def RET_model(taxo: Taxonomy, query: str, k: int=10) -> List[Hashable]:\n",
    "    topk = ret_model.search(query, top_k=k)\n",
    "    return [idx_dict[i] for i,_,_ in topk]\n",
    "\n",
    "def GEN_model(labels: List[str], prefix='summarize: ') -> str:\n",
    "    corpus = prefix\n",
    "    for l in labels:\n",
    "        corpus += l + '; '\n",
    "    corpus = corpus[:-2]\n",
    "    inputs = gen_tokenizer(corpus,return_tensors='pt').to(device)['input_ids']\n",
    "    outputs = gen_model.generate(inputs,max_length=64)[0]\n",
    "    decoded = gen_tokenizer.decode(outputs.cpu().numpy(),skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "def SUB_model(sub: Union[str, List[str]], sup: Union[str, List[str]], batch_size :int=256) -> np.ndarray:\n",
    "    if isinstance(sub, str):\n",
    "        sub, sup = [sub], [sup]\n",
    "    if len(sub) <= batch_size:\n",
    "        inputs = sub_tokenizer(sub,sup,padding=True,return_tensors='pt').to(device)\n",
    "        predictions = torch.softmax(sub_model(**inputs).logits.detach().cpu(),1)[:,1].numpy()\n",
    "    else:\n",
    "        head = (sub[:batch_size], sup[:batch_size])\n",
    "        tail = (sub[batch_size:],sup[batch_size:])\n",
    "        predictions = np.concatenate((SUB_model(head[0], head[1], batch_size=batch_size), SUB_model(tail[0], tail[1], batch_size=batch_size)))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682f1b5",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Almost there! Configure your run by specifying the data, models and settings. Check [here](/README.md#configurations) to see how to choose the right settings for your purpose. \n",
    "\n",
    "In the following example, we will run auto mode with 10 outer loops. We will also set `logging` to `True` to see a detailed logging of ICON's actions and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fac59b-a9f5-4e24-a463-160967c6401b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kwargs = {'data': taxo,\n",
    "        'ret_model': RET_model,\n",
    "        'gen_model': GEN_model,\n",
    "        'sub_model': SUB_model,\n",
    "        'max_outer_loop': 10,\n",
    "        'restrict_combinations': False,\n",
    "        'retrieve_size': 5,\n",
    "        'logging': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e371c9f",
   "metadata": {},
   "source": [
    "## Running\n",
    "\n",
    "We have prepared everything to run ICON. Simply initialise an ICON object with our configuration and call `run()`. \n",
    "\n",
    "If you change your mind on the settings before running, you don't have to initialise again: calling `update_config` would suffice.\n",
    "\n",
    "The output of a run will be either a new taxonomy (as is the case here) or a list of ICON predictions. To save a taxonomy to a file, use the `to_json` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8edbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "iconobj = ICON(**kwargs)\n",
    "iconobj.update_config(threshold=0.9) # Example of updating configurations\n",
    "outputs = iconobj.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
